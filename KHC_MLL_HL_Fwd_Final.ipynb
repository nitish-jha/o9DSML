{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb457a6-682c-4213-80b6-bf1ad9df49ca",
   "metadata": {
    "editable": false
   },
   "outputs": [],
   "source": [
    "# This cell is NOT editable. Overwrite variables on your own discretion.\n",
    "# Any changes other than the script code will NOT BE SAVED!\n",
    "# All cells are assumed to be script code cells, unless explictly tagged as 'o9_ignore'\n",
    "\n",
    "f_wk = \"114\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "646cd3e4-5cdf-43ad-a0a1-31eb2034391b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Slice Dimension Attributes defined in the plugin. Please check all queries and replace <KEY HERE> with a valid name.\n",
    "# For example: If slice is defined by Version.[Version Name] and Time.[Month]\n",
    "# input_df = ibpl Select ([Version].[Version Name].[<KEY HERE>] * [Time].[Month].[<KEY HERE>] * [Item].[Item Number]) on row, ({Measure.[M1], Measure.[M2]}) on column limit 5000;\n",
    "#                             update <KEY HERE> to valid names\n",
    "# input_df = ibpl Select ([Version].[Version Name].[CurrentWorkingView] * [Time].[Month].[January] * [Item].[Item Number]) on row, ({Measure.[M1], Measure.[M2]}) on column limit 5000;\n",
    "\n",
    "_tran_df = \"Select ([Version].[Version Name] * [Sales Domain].[Sales Org] * [Sales Domain].[Customer Group] * [Location].[Location] *  [EPM].[EPM] * [Time].[Week] * [Item].[L6].[<KEY HERE>] * [Item].[Planning Item] ) on row, ({Measure.[Actual Shifted]}) on column;\"\n",
    "_prod_df = \"Select ([Item].[Item] * [Item].[L3] * [Item].[L4] * [Item].[L5] * [Item].[L6].[<KEY HERE>]);\"\n",
    "_loc_df = \"Select ([Location].[Location] * [Location].[Location Type] * [Location].[Location Region]);\"\n",
    "_time_df = \"Select ([Time].[Day] * [Time].[Week]);\"\n",
    "_week_df = \"Select (&CurrentWeek.element(0)*[Version].[Version Name]);\"\n",
    "_calendar_df = \"Select ([Version].[Version Name] * [Time].[Week] * [Sales Domain].[Country] ) on row, ({Measure.[Holiday Type], Measure.[Is Holiday]}) on column;\"\n",
    "_segment_df = \"Select ([Version].[Version Name] * [EPM].[Category] * [Sales Domain].[Sales Org] * [Item].[Planning Item] ) on row,  ({Measure.[Product Segment]}) on column;\"\n",
    "\n",
    "\n",
    "# Initialize the O9DataLake with the input parameters and dataframes\n",
    "# Data can be accessed with O9DataLake.get(<Input Name>)\n",
    "# Overwritten values will not be reflected in the O9DataLake after initialization\n",
    "\n",
    "from o9_common_utils.O9DataLake import O9DataLake, ResourceType, DataSource\n",
    "O9DataLake.register(\"tran_df\",DataSource.LS, ResourceType.IBPL, _tran_df)\n",
    "O9DataLake.register(\"prod_df\",DataSource.LS, ResourceType.IBPL, _prod_df)\n",
    "O9DataLake.register(\"loc_df\",DataSource.LS, ResourceType.IBPL, _loc_df)\n",
    "O9DataLake.register(\"time_df\",DataSource.LS, ResourceType.IBPL, _time_df)\n",
    "O9DataLake.register(\"week_df\",DataSource.LS, ResourceType.IBPL, _week_df)\n",
    "O9DataLake.register(\"calendar_df\",DataSource.LS, ResourceType.IBPL, _calendar_df)\n",
    "O9DataLake.register(\"segment_df\",DataSource.LS, ResourceType.IBPL, _segment_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f88fe9d-7b86-4ef3-a4bc-300f12139427",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "#####--IMPORTS--#####\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "from datetime import datetime, timedelta, date\n",
    "\n",
    "import gc\n",
    "\n",
    "import sklearn\n",
    "from o9_common_utils.O9DataLake import O9DataLake\n",
    "from xgboost import XGBRFRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "import time\n",
    "import category_encoders as ce\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.filterwarnings(action='ignore', category=UserWarning)\n",
    "\n",
    "################################################################################\n",
    "#####--PARAMETERS--#####\n",
    "\n",
    "cwd = os.getcwd().split(\":\")[0]\n",
    "\n",
    "if cwd in [\"C\", \"D\", \"E\", \"F\"]:\n",
    "    env = \"local\"\n",
    "\n",
    "else:\n",
    "    env = \"tenant\"\n",
    "    logger = logging.getLogger(\"o9_logger\")\n",
    "    logger.info(\"Environment:\")\n",
    "    logger.info(env)\n",
    "\n",
    "pd.set_option('display.max_columns', 50)\n",
    "\n",
    "#####Define the starting lag2 week and backtest cycles####\n",
    "# current_week = '2021-W43'\n",
    "# cycles = 5\n",
    "\n",
    "\"\"\"backtest here for 8 cycles. So cycle would end for week 2021-W34 which means \n",
    "we will be in W31 and have data till W30\"\"\"\n",
    "\n",
    "###Set number of weeks of forecast (forecast length)\n",
    "try:\n",
    "    f_wk = int(f_wk)\n",
    "except:\n",
    "    logger.debug(\n",
    "        \"Integer value not entered for f_wk, taking default value of 114\")\n",
    "    f_wk = 114\n",
    "\n",
    "if f_wk > 114:\n",
    "    f_wk = 114\n",
    "\n",
    "################################################################################\n",
    "#####--DATA IMPORT--#####\n",
    "\n",
    "###passing proper datatypes for import###\n",
    "tran_df_dtypes = {\n",
    "    \"Qty\": \"float\",\n",
    "    \"Location\": \"str\",\n",
    "    \"Item\": \"str\",\n",
    "    \"Customer\": \"str\",\n",
    "    \"Sales Org\": \"str\"\n",
    "}\n",
    "\n",
    "prod_df_dtypes = {\n",
    "    \"Item\": \"str\",\n",
    "    \"L6\": \"str\"\n",
    "}\n",
    "\n",
    "loc_df_dtypes = {\n",
    "    \"Location\": \"str\"\n",
    "}\n",
    "\n",
    "if env == \"local\":\n",
    "\n",
    "    reg = 'US'\n",
    "\n",
    "    # cwd = 'D:/o9/Kraft Heinz/ML/HL MLL Code/'\n",
    "\n",
    "    cwd = 'C:/Users/gaurang.makhaira/OneDrive - o9 Solutions/Documents/Kraft Heinz/'\n",
    "\n",
    "    os.chdir(cwd)\n",
    "\n",
    "    ###getting actual shifted values in tran_df###\n",
    "    tran_df = pd.read_csv(cwd + 'shipment1.csv', dtype=str)\n",
    "    tran_df.columns = ['Version', 'Sales Org', 'Customer', 'Location', 'EPM',\n",
    "                       'WeekStr', 'L6', 'Item', 'Qty']\n",
    "\n",
    "    version = tran_df['Version'].unique()[0]\n",
    "    country = tran_df['Sales Org'].unique()[0][:2]\n",
    "    salesorg = tran_df['Sales Org'].unique()[0]\n",
    "    cat = tran_df['L6'].unique()[0]\n",
    "    del tran_df['Version']\n",
    "    tran_df = tran_df.astype(tran_df_dtypes)\n",
    "\n",
    "    ###getting product master data###\n",
    "    prod_df = pd.read_csv(cwd + 'prod1.csv', dtype=str)\n",
    "    prod_df = prod_df[\n",
    "        ['Item.[Item]', 'Item.[L3]', 'Item.[L4]', 'Item.[L5]', 'Item.[L6]']]\n",
    "    prod_df.columns = ['Item', 'L3', 'L4', 'L5', 'L6']\n",
    "    # prod_df = prod_df.astype(prod_df_dtypes)\n",
    "\n",
    "    ###getting location master data###\n",
    "    loc_df = pd.read_csv(cwd + 'loc1.csv', dtype=str)\n",
    "    loc_df = loc_df[['Location.[Location]', 'Location.[Location Type]',\n",
    "                     'Location.[Location Region]']]\n",
    "    loc_df.columns = ['Location', 'LocType', 'Region']\n",
    "    # loc_df = loc_df.astype(loc_df_dtypes)\n",
    "\n",
    "    ###getting time master data###\n",
    "    time_df = pd.read_csv(cwd + 'time1.csv')\n",
    "    # time_df = pd.read_csv(cwd + 'Time.csv')\n",
    "\n",
    "    temp = time_df.groupby('Time.[Week]').first()\n",
    "    time_df = pd.merge(temp['Time.[Day]'], time_df, on='Time.[Day]', how='left')\n",
    "\n",
    "    time_df = time_df[['Time.[Week]', 'Time.[Day]']]\n",
    "    time_df['Time.[Day]'] = pd.to_datetime(time_df['Time.[Day]'],\n",
    "                                           format='%d-%b-%Y')\n",
    "    # time_df['Time.[Day]'] = pd.to_datetime(b['Time.[Day]'], format='%Y-%b-%d')\n",
    "\n",
    "    # time_df = time_df[['Time.[Week]', 'Time.[WeekKey]']]\n",
    "    time_df.columns = ['WeekStr', 'Week']\n",
    "    time_df['Week'] = pd.to_datetime(time_df['Week'])\n",
    "    time_df = time_df.drop_duplicates()\n",
    "\n",
    "    ###getting current week###\n",
    "    week_df = pd.read_csv(cwd + 'weekflag1.csv')\n",
    "    weeklist_bt_df_1 = week_df.copy()\n",
    "\n",
    "    weeklist_bt_df_1['Time.[Week]'] = weeklist_bt_df_1[\n",
    "        'Time.[Week]'].str.replace('-W', '')\n",
    "\n",
    "    current_week = weeklist_bt_df_1['Time.[Week]'].unique()[0]\n",
    "    current_week = int(current_week)\n",
    "\n",
    "    ###getting holiday data###\n",
    "    calendar_df = pd.read_csv(cwd + 'holiday1.csv')\n",
    "    calendar_df = calendar_df.sort_values(['Time.[Week]'])\n",
    "\n",
    "    calendar_df = calendar_df[calendar_df['Sales Domain.[Country]'] == country]\n",
    "    calendar_df.rename(columns={'Time.[Week]': 'WEEK_NUM'}, inplace=True)\n",
    "\n",
    "    calendar_df['WEEKStr'] = calendar_df['WEEK_NUM']\n",
    "\n",
    "    calendar_df['WEEK_NUM'] = calendar_df['WEEK_NUM'].str.replace('-W', '')\n",
    "    calendar_df = calendar_df.pivot_table(index=['WEEK_NUM', 'WEEKStr'],\n",
    "                                          columns='Holiday Type',\n",
    "                                          values='Is Holiday', aggfunc='mean',\n",
    "                                          fill_value=0).reset_index()\n",
    "    calendar_df = calendar_df.rename_axis(None, axis=1)\n",
    "\n",
    "    for i in calendar_df.columns:\n",
    "        if 'WEEK' not in i:\n",
    "            calendar_df.rename(columns={i: 'holiday_' + i}, inplace=True)\n",
    "\n",
    "    calendar_df['WEEK_NUM'] = calendar_df['WEEK_NUM'].astype(np.int64)\n",
    "\n",
    "    cal_cols = list(calendar_df.columns)\n",
    "    cal_cols.remove('WEEK_NUM')\n",
    "    cal_cols.remove('WEEKStr')\n",
    "\n",
    "    calendar_df.dtypes\n",
    "\n",
    "    calendar_df['holidaytype'] = calendar_df[cal_cols].idxmax(axis=1)\n",
    "\n",
    "    calendar_df['Total_Holidays'] = calendar_df[cal_cols].sum(axis=1)\n",
    "\n",
    "    calendar_df['holidayflag'] = np.where(calendar_df['Total_Holidays'] >= 1,\n",
    "                                          1, 0)\n",
    "\n",
    "    calendar_df.drop(cal_cols, axis=1, inplace=True)\n",
    "    calendar_df.drop(['Total_Holidays', 'WEEK_NUM'], axis=1, inplace=True)\n",
    "    calendar_df.rename({'WEEKStr': 'WeekStr'}, axis=1, inplace=True)\n",
    "\n",
    "    calendar_df['holidaytype'] = calendar_df['holidaytype'].str.replace(\n",
    "        'holiday_', '')\n",
    "    calendar_df['holidaytype'] = calendar_df['holidaytype'].str.replace(\n",
    "        'Easter,Easter Monday',\n",
    "        'Easter')\n",
    "\n",
    "    calendar_df = pd.merge(calendar_df, time_df, how='left', on='WeekStr')\n",
    "\n",
    "    ###getting segment data###\n",
    "    segment_df = pd.read_csv(cwd + 'segment1.csv', dtype=str)\n",
    "    segment_df = segment_df[segment_df['Sales Domain.[Sales Org]'] == salesorg]\n",
    "\n",
    "    segment_df.rename({'Version.[Version Name]': 'Version',\n",
    "                       'Sales Domain.[Sales Org]': 'Sales Org',\n",
    "                       'EPM.[Category]': 'EPM_Cat',\n",
    "                       'Item.[Planning Item]': 'Item'},\n",
    "                      axis=1, inplace=True)\n",
    "\n",
    "    del segment_df['Version']\n",
    "\n",
    "    print(\"current_week\")\n",
    "    print(current_week)\n",
    "\n",
    "    print(\"tran_df : {}\".format(tran_df.shape))\n",
    "    print(\"prod_df : {}\".format(prod_df.shape))\n",
    "    print(\"loc_df : {}\".format(loc_df.shape))\n",
    "    print(\"time_df : {}\".format(time_df.shape))\n",
    "    print(\"current_week: {}\".format(current_week))\n",
    "    print(\"time df head\")\n",
    "    print(time_df.head())\n",
    "    print(\"calendar_df cols: {}\".format(calendar_df.columns))\n",
    "\n",
    "\n",
    "else:\n",
    "\n",
    "    ###getting actual shifted values in tran_df###\n",
    "    logger.debug(\"tran df head\")\n",
    "    logger.debug(tran_df.head())\n",
    "    logger.debug(tran_df.dtypes)\n",
    "\n",
    "    tran_df.rename({'Version.[Version Name]': 'Version',\n",
    "                    'Sales Domain.[Sales Org]': 'Sales Org',\n",
    "                    'Sales Domain.[Customer Group]': 'Customer',\n",
    "                    'Location.[Location]': 'Location', 'EPM.[EPM]': 'EPM',\n",
    "                    'Time.[Week]': 'WeekStr', 'Item.[L6]': 'L6',\n",
    "                    'Item.[Planning Item]': 'Item', 'Actual Shifted': 'Qty'},\n",
    "                   axis=1, inplace=True)\n",
    "\n",
    "    version = tran_df['Version'].unique()[0]\n",
    "    country = tran_df['Sales Org'].unique()[0][:2]\n",
    "    salesorg = tran_df['Sales Org'].unique()[0]\n",
    "    cat = tran_df['L6'].unique()[0]\n",
    "    del tran_df['Version']\n",
    "    tran_df = tran_df.astype(tran_df_dtypes)\n",
    "\n",
    "    ###getting product master data###\n",
    "    logger.debug(\"prod df head\")\n",
    "    logger.debug(prod_df.head())\n",
    "    logger.debug(prod_df.dtypes)\n",
    "\n",
    "    prod_df.rename({'Item.[Item]': 'Item', 'Item.[L3]': 'L3', 'Item.[L4]': 'L4',\n",
    "                    'Item.[L5]': 'L5', 'Item.[L6]': 'L6'}, axis=1, inplace=True)\n",
    "    # prod_df = prod_df.astype(prod_df_dtypes)\n",
    "\n",
    "    ###getting location master data###\n",
    "    logger.debug(\"loc df head\")\n",
    "    logger.debug(loc_df.head())\n",
    "    logger.debug(loc_df.dtypes)\n",
    "\n",
    "    loc_df.rename({'Location.[Location]': 'Location',\n",
    "                   'Location.[Location Type]': 'LocType',\n",
    "                   'Location.[Location Region]': 'Region'}, axis=1,\n",
    "                  inplace=True)\n",
    "    # loc_df = loc_df.astype(loc_df_dtypes)\n",
    "\n",
    "    ###getting time master data###\n",
    "\n",
    "    logger.debug(\"time df head\")\n",
    "    logger.debug(time_df.head())\n",
    "    logger.debug(time_df.dtypes)\n",
    "\n",
    "    temp = time_df.groupby('Time.[Week]').first()\n",
    "    time_df = pd.merge(temp['Time.[Day]'], time_df, on='Time.[Day]', how='left')\n",
    "\n",
    "    time_df = time_df[['Time.[Week]', 'Time.[Day]']]\n",
    "    time_df['Time.[Day]'] = pd.to_datetime(time_df['Time.[Day]'],\n",
    "                                           format='%d-%b-%Y')\n",
    "\n",
    "    time_df.rename({'Time.[Week]': 'WeekStr', 'Time.[Day]': 'Week'}, axis=1,\n",
    "                   inplace=True)\n",
    "    time_df['Week'] = pd.to_datetime(time_df['Week'])\n",
    "    time_df = time_df.drop_duplicates()\n",
    "    print(time_df[time_df['WeekStr'].isna()])\n",
    "\n",
    "    ###getting current week###\n",
    "    weeklist_bt_df_1 = week_df.copy()\n",
    "\n",
    "    weeklist_bt_df_1['Time.[Week]'] = weeklist_bt_df_1[\n",
    "        'Time.[Week]'].str.replace('-W', '')\n",
    "\n",
    "    current_week = weeklist_bt_df_1['Time.[Week]'].unique()[0]\n",
    "    current_week = int(current_week)\n",
    "\n",
    "    ###getting holiday data###\n",
    "    # calendar_df = pd.read_csv(cwd + 'holiday.csv')\n",
    "    calendar_df = calendar_df.sort_values(['Time.[Week]'])\n",
    "\n",
    "    calendar_df = calendar_df[calendar_df['Sales Domain.[Country]'] == country]\n",
    "    calendar_df.rename(columns={'Time.[Week]': 'WEEK_NUM'}, inplace=True)\n",
    "\n",
    "    calendar_df['WEEKStr'] = calendar_df['WEEK_NUM']\n",
    "\n",
    "    calendar_df['WEEK_NUM'] = calendar_df['WEEK_NUM'].str.replace('-W', '')\n",
    "    calendar_df = calendar_df.pivot_table(index=['WEEK_NUM', 'WEEKStr'],\n",
    "                                          columns='Holiday Type',\n",
    "                                          values='Is Holiday', aggfunc='mean',\n",
    "                                          fill_value=0).reset_index()\n",
    "    calendar_df = calendar_df.rename_axis(None, axis=1)\n",
    "\n",
    "    for i in calendar_df.columns:\n",
    "        if 'WEEK' not in i:\n",
    "            calendar_df.rename(columns={i: 'holiday_' + i}, inplace=True)\n",
    "\n",
    "    calendar_df['WEEK_NUM'] = calendar_df['WEEK_NUM'].astype(np.int64)\n",
    "\n",
    "    cal_cols = list(calendar_df.columns)\n",
    "    cal_cols.remove('WEEK_NUM')\n",
    "    cal_cols.remove('WEEKStr')\n",
    "\n",
    "    calendar_df['holidaytype'] = calendar_df[cal_cols].idxmax(axis=1)\n",
    "\n",
    "    calendar_df['Total_Holidays'] = calendar_df[cal_cols].sum(axis=1)\n",
    "\n",
    "    calendar_df['holidayflag'] = np.where(calendar_df['Total_Holidays'] >= 1,\n",
    "                                          1, 0)\n",
    "\n",
    "    calendar_df.drop(cal_cols, axis=1, inplace=True)\n",
    "    calendar_df.drop(['Total_Holidays', 'WEEK_NUM'], axis=1, inplace=True)\n",
    "    calendar_df.rename({'WEEKStr': 'WeekStr'}, axis=1, inplace=True)\n",
    "\n",
    "    calendar_df['holidaytype'] = calendar_df['holidaytype'].str.replace(\n",
    "        'holiday_', '')\n",
    "    calendar_df['holidaytype'] = calendar_df['holidaytype'].str.replace(\n",
    "        'Easter,Easter Monday',\n",
    "        'Easter')\n",
    "\n",
    "    calendar_df = pd.merge(calendar_df, time_df, how='left', on='WeekStr')\n",
    "\n",
    "    ###getting segment data###\n",
    "    # segment_df = pd.read_csv(cwd + 'segment.csv', dtype=str)\n",
    "    segment_df = segment_df[segment_df['Sales Domain.[Sales Org]'] == salesorg]\n",
    "\n",
    "    segment_df.rename({'Version.[Version Name]': 'Version',\n",
    "                       'Sales Domain.[Sales Org]': 'Sales Org',\n",
    "                       'EPM.[Category]': 'EPM_Cat',\n",
    "                       'Item.[Planning Item]': 'Item'},\n",
    "                      axis=1, inplace=True)\n",
    "\n",
    "    del segment_df['Version']\n",
    "\n",
    "    logger.debug(\"current_week\")\n",
    "    logger.debug(current_week)\n",
    "\n",
    "    logger.debug(\"tran_df : {}\".format(tran_df.shape))\n",
    "    logger.debug(\"tran df head\")\n",
    "    logger.debug(tran_df.head())\n",
    "    logger.debug(tran_df.dtypes)\n",
    "\n",
    "    logger.debug(\"prod_df : {}\".format(prod_df.shape))\n",
    "    logger.debug(\"loc_df : {}\".format(loc_df.shape))\n",
    "    logger.debug(\"time_df : {}\".format(time_df.shape))\n",
    "    logger.debug(\"Current week : {}\".format(current_week))\n",
    "    logger.debug(\"time df head\")\n",
    "    logger.debug(time_df.head())\n",
    "    logger.debug(time_df.dtypes)\n",
    "    logger.debug(\"calendar df\")\n",
    "    logger.debug(calendar_df.head())\n",
    "    logger.debug(\"segment df\")\n",
    "    logger.debug(segment_df.head())\n",
    "\n",
    "################################################################################\n",
    "#####--EXECUTION--#####\n",
    "\n",
    "###Aggregating to Sales Org level###\n",
    "tran_df2 = tran_df.groupby(['Sales Org', 'Location',\n",
    "                            'Item', 'WeekStr'])['Qty'].sum().reset_index()\n",
    "\n",
    "if env != \"local\":\n",
    "    logger.debug(\"tran df2: \")\n",
    "    logger.debug(tran_df2.head())\n",
    "    logger.debug(tran_df2.dtypes)\n",
    "\n",
    "##Joining Week to Actuals df##\n",
    "tran_df2 = pd.merge(tran_df2, time_df, how='left', on='WeekStr')\n",
    "tran_df = pd.merge(tran_df, time_df, how='left', on='WeekStr')\n",
    "\n",
    "# tran_df2 = pd.merge(tran_df2, temp_df, how = 'left', on = 'WeekStr')\n",
    "# tran_df = pd.merge(tran_df, temp_df, how = 'left', on = 'WeekStr')\n",
    "#\n",
    "# tran_df2['holidaytype'] = tran_df2['holidaytype'].fillna(0)\n",
    "# tran_df2['holidayflag'] = tran_df2['holidayflag'].fillna(0)\n",
    "# tran_df['holidaytype'] = tran_df['holidaytype'].fillna(0)\n",
    "# tran_df['holidayflag'] = tran_df['holidayflag'].fillna(0)\n",
    "#\n",
    "# holiday_col = [col for col in tran_df2 if col.startswith('holiday')]\n",
    "#\n",
    "# for col in holiday_col:\n",
    "#     tran_df2[col+'_lead1'] = tran_df2.groupby(['Item','Location',\n",
    "#                                                'Sales Org'])[col].shift(-1)\n",
    "#     tran_df2[col+'_lead2'] = tran_df2.groupby(['Item','Location',\n",
    "#                                                'Sales Org'])[col].shift(-2)\n",
    "#     tran_df2[col+'_lead3'] = tran_df2.groupby(['Item','Location',\n",
    "#                                                'Sales Org'])[col].shift(-3)\n",
    "#\n",
    "# tran_df2.fillna(0)\n",
    "\n",
    "if env != \"local\":\n",
    "    logger.debug(\"tran df2 after merger with time df: \")\n",
    "    logger.debug(tran_df2.head())\n",
    "    logger.debug(tran_df2.dtypes)\n",
    "\n",
    "    logger.debug(\"tran df after merger with time df: \")\n",
    "    logger.debug(tran_df.head())\n",
    "    logger.debug(tran_df.dtypes)\n",
    "\n",
    "time_df.dropna(inplace=True)\n",
    "time_df['WEEK'] = time_df['WeekStr'].str.replace('-W', '')\n",
    "time_df['WEEK'] = time_df['WEEK'].astype('int64')\n",
    "\n",
    "tran_df2_orig = tran_df2.copy()\n",
    "tran_df_orig = tran_df.copy()\n",
    "\n",
    "############Start loop for backtesting here#################\n",
    "max_week = time_df[time_df['WEEK'] == current_week].Week.max()\n",
    "ship_max_week = current_week - 1\n",
    "\n",
    "if env != \"local\":\n",
    "    logger.debug(\"Max week for backtesting: \")\n",
    "    logger.debug(max_week)\n",
    "\n",
    "start = max_week - np.timedelta64(1, 'W')\n",
    "\n",
    "if env != \"local\":\n",
    "    logger.debug(\"start: \")\n",
    "    logger.debug(start)\n",
    "\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "\n",
    "# for c in range(1, cycles + 1):\n",
    "#     c = 1\n",
    "\n",
    "hist_week = start\n",
    "if env != \"local\":\n",
    "    logger.debug(\"hist week: \")\n",
    "    logger.debug(hist_week)\n",
    "\n",
    "# if env == \"local\":\n",
    "#     print(str(c) + \" out of \" + str(cycles) + \" : \" + str(hist_week))\n",
    "# else:\n",
    "#     logger.debug(str(c) + \" out of \" + str(cycles) + \" : \" + str(hist_week))\n",
    "\n",
    "####Filtering out data per the bcktest cycle####\n",
    "tran_df2 = tran_df2_orig[tran_df2_orig['Week'] <= hist_week]\n",
    "tran_df = tran_df_orig[tran_df_orig['Week'] <= hist_week]\n",
    "\n",
    "## Filtering out SKU with 0 volume in last 56 weeks ##\n",
    "EOL_Date = tran_df2['Week'].max() - np.timedelta64(7 * 56, 'D')\n",
    "Valid_SKUs = tran_df2[tran_df2['Week'] > EOL_Date]\n",
    "Valid_SKUs = Valid_SKUs.groupby(['Sales Org', 'Location', 'Item'])[\n",
    "    'Qty'].sum().reset_index()\n",
    "Valid_SKUs = Valid_SKUs[Valid_SKUs['Qty'] > 0]\n",
    "Valid_SKUs['Key'] = Valid_SKUs['Sales Org'] + '-' + Valid_SKUs[\n",
    "    'Location'] + '-' + Valid_SKUs['Item']\n",
    "\n",
    "if env != \"local\":\n",
    "    logger.debug(\"Valid_SKUs : {}\".format(Valid_SKUs.shape))\n",
    "\n",
    "tran_df2['Key'] = tran_df2['Sales Org'] + '-' + tran_df2['Location'] + '-' + \\\n",
    "                  tran_df2['Item']\n",
    "tran_df2 = tran_df2[tran_df2['Key'].isin(Valid_SKUs['Key'])]\n",
    "del tran_df2['Key']\n",
    "\n",
    "if env != \"local\":\n",
    "    logger.debug(\"tran_df2 : {}\".format(tran_df2.shape))\n",
    "    logger.debug(tran_df2.head())\n",
    "\n",
    "#### Aggregating filling holes #####\n",
    "df1 = tran_df2.groupby(['Sales Org', 'Location', 'Item', 'Week'])[\n",
    "    'Qty'].sum()\n",
    "\n",
    "if env != \"local\":\n",
    "    logger.debug(\"df1 : {}\".format(df1.shape))\n",
    "    logger.debug(df1.head())\n",
    "\n",
    "## Filling holes by pivot melt. Can also do stack/unstack or reindex ##\n",
    "\n",
    "df1_StartingDate = tran_df2.groupby(['Sales Org', 'Location', 'Item'])[\n",
    "    'Week'].min().reset_index()\n",
    "df1_StartingDate = df1_StartingDate.rename(\n",
    "    columns={'Week': 'Starting_Week'})\n",
    "\n",
    "if env != \"local\":\n",
    "    logger.debug(\"df1_StartingDate : {}\".format(df1_StartingDate.shape))\n",
    "    logger.debug(df1_StartingDate.head())\n",
    "\n",
    "df1 = df1.unstack().stack(dropna=False).fillna(0).astype(int)\n",
    "df1 = df1.reset_index()\n",
    "df1 = df1.rename(columns={0: 'Qty'})\n",
    "\n",
    "df1 = pd.merge(df1, df1_StartingDate, how='left',\n",
    "               on=['Sales Org', 'Location', 'Item'])\n",
    "df1 = df1[df1['Week'] >= df1['Starting_Week']]\n",
    "# df1.info()\n",
    "\n",
    "if env != \"local\":\n",
    "    logger.debug(\"df1 after stack unstack\")\n",
    "    logger.debug(df1.shape)\n",
    "\n",
    "########Joining#######\n",
    "## Joining to get dimension attributes ##\n",
    "df1 = pd.merge(df1, prod_df, how='left', on='Item')\n",
    "df1 = pd.merge(df1, loc_df, how='left', on='Location')\n",
    "\n",
    "###Creating Item_Loc Key###\n",
    "df1['Item_Loc'] = df1['Item'] + '_' + df1['Location']\n",
    "\n",
    "if env != \"local\":\n",
    "    logger.debug(\"df1 after joining the dimension attributes\")\n",
    "    logger.debug(df1.shape)\n",
    "\n",
    "\n",
    "##### Feature Engineering #####\n",
    "\n",
    "### Creating Time Features ###\n",
    "def time_features(df):\n",
    "    df['Year_Nominal'] = df['Week'].dt.year\n",
    "    df['Year_Num'] = df['Year_Nominal'] - df['Year_Nominal'].min() + 1\n",
    "    # df['Year_Nominal'] = df['Year_Nominal'].astype(str)\n",
    "\n",
    "    df['Quarter_Num'] = df['Week'].dt.quarter\n",
    "    df['Quarter_Sin'] = np.sin(2 * np.pi * (df['Quarter_Num'] - 1) / 4)\n",
    "    df['Quarter_Cos'] = np.cos(2 * np.pi * (df['Quarter_Num'] - 1) / 4)\n",
    "    # df['Quarter_Nominal'] = df['Quarter_Num'].astype(str)\n",
    "\n",
    "    df['Month_Num'] = df['Week'].dt.month\n",
    "    df['Month_Sin'] = np.sin(2 * np.pi * (df['Month_Num'] - 1) / 12)\n",
    "    df['Month_Cos'] = np.cos(2 * np.pi * (df['Month_Num'] - 1) / 12)\n",
    "    df['Month_Count'] = ((df['Week'] - df['Week'].min()) /\n",
    "                         np.timedelta64(1, 'M')).astype(int)\n",
    "    # df['Month_Nominal'] = df['Month_Num'].astype(str)\n",
    "\n",
    "    df['Week_Num'] = df['Week'].dt.week\n",
    "    df['Week_Sin'] = np.sin(2 * np.pi * (df['Month_Num'] - 1) / 52)\n",
    "    df['Week_Cos'] = np.cos(2 * np.pi * (df['Month_Num'] - 1) / 52)\n",
    "    # df['Week_Nominal'] = df['Week_Num'].astype(str)\n",
    "    df['WOM_Num'] = df['Week'].apply(lambda d: (d.day - 1) // 7 + 1)\n",
    "    # df['WOM_Nominal'] = df['WOM_Num'].astype(str)\n",
    "\n",
    "    df['Year_End'] = np.where(df['Week_Num'] >= 51, 1, 0)\n",
    "    # temp = df1[['Year_End', 'Day', 'Christmas', 'Holiday']]\n",
    "\n",
    "    if env != \"local\":\n",
    "        logger.debug(\"df after creating time features\")\n",
    "        logger.debug(df.shape)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "df1 = time_features(df1)\n",
    "\n",
    "\n",
    "######Creating aggregated POS features########\n",
    "def agg_features(df):\n",
    "    df = df.join(df.groupby(['Item', 'Week'])['Qty'].mean(),\n",
    "                 on=['Item', 'Week'], rsuffix='_Item')\n",
    "    df = df.join(df.groupby(['Location', 'Week'])['Qty'].mean(),\n",
    "                 on=['Location', 'Week'], rsuffix='_Location')\n",
    "    df = df.join(df.groupby(['L5', 'Week'])['Qty'].mean(),\n",
    "                 on=['L5', 'Week'], rsuffix='_L5')\n",
    "    df = df.join(df.groupby(['LocType', 'Week'])['Qty'].mean(),\n",
    "                 on=['LocType', 'Week'], rsuffix='_LocType')\n",
    "\n",
    "    if env != \"local\":\n",
    "        logger.debug(\"df after creating aggregated POS features\")\n",
    "        logger.debug(df.shape)\n",
    "\n",
    "    return (df)\n",
    "\n",
    "\n",
    "df1 = agg_features(df1)\n",
    "\n",
    "\n",
    "######Creating lag variables########\n",
    "def lag_features_fn(df, col_index, col_time, col_shift, lag_start=1,\n",
    "                    lag_end=9, ly_start=51, ly_end=53, lly_start=103,\n",
    "                    lly_end=105):\n",
    "    # df = test_df\n",
    "    Data_For_Shift = df[col_index + col_time + col_shift]\n",
    "    Data_For_Shift = Data_For_Shift.set_index(col_index + col_time)\n",
    "\n",
    "    Data_Shifted = df[col_index + col_time]\n",
    "    Data_Shifted = Data_Shifted.set_index(col_index + col_time)\n",
    "\n",
    "    for i in range(lag_start, lag_end + 1):\n",
    "        Data_Shift = Data_For_Shift.groupby(level=col_index).shift(i)\n",
    "        Data_Shifted = Data_Shifted.join(\n",
    "            Data_Shift.rename(columns=lambda x: x + \"_Lag\" + str(i)))\n",
    "\n",
    "    for i in range(ly_start, ly_end + 1):\n",
    "        Data_Shift = Data_For_Shift.groupby(level=col_index).shift(i)\n",
    "        Data_Shifted = Data_Shifted.join(\n",
    "            Data_Shift.rename(columns=lambda x: x + \"_Lag\" + str(i)))\n",
    "\n",
    "    for i in range(lly_start, lly_end + 1):\n",
    "        Data_Shift = Data_For_Shift.groupby(level=col_index).shift(i)\n",
    "        Data_Shifted = Data_Shifted.join(\n",
    "            Data_Shift.rename(columns=lambda x: x + \"_Lag\" + str(i)))\n",
    "\n",
    "    Data_Shifted = Data_Shifted.reset_index()\n",
    "    Data_Shifted = Data_Shifted.reindex(sorted(Data_Shifted.columns),\n",
    "                                        axis=1)\n",
    "\n",
    "    df = pd.merge(df, Data_Shifted, how='left', on=(col_index + col_time))\n",
    "\n",
    "    if env != \"local\":\n",
    "        logger.debug(\"df after creating lag features\")\n",
    "        logger.debug(df.shape)\n",
    "\n",
    "    return (df)\n",
    "\n",
    "\n",
    "col_shift = ['Qty', 'Qty_Item', 'Qty_Location', 'Qty_L5', 'Qty_LocType']\n",
    "col_index = ['Sales Org', 'Location', 'Item']\n",
    "col_time = ['Week']\n",
    "lag_start = 4\n",
    "lag_end = 9\n",
    "ly_start = 51\n",
    "ly_end = 53\n",
    "lly_start = 103\n",
    "lly_end = 105\n",
    "\n",
    "df2 = lag_features_fn(df1, col_index, col_time, col_shift, lag_start, lag_end,\n",
    "                      ly_start, ly_end, lly_start, lly_end)\n",
    "\n",
    "\n",
    "# def ly_fill(df, col, ly_start, ly_end, ly_shift = 52):\n",
    "#     for i in range(int(ly_start), int(ly_end+1)):\n",
    "#         df[col+'_Lag'+str(i+ly_shift)].fillna(df[col+'_Lag'+str(i)], inplace=True)\n",
    "\n",
    "# ly_fill(df2, 'Qty', ly_start, ly_end, 52)\n",
    "# ly_fill(df2, 'Qty_Item', ly_start, ly_end, 52)\n",
    "# ly_fill(df2, 'Qty_Location', ly_start, ly_end, 52)\n",
    "# ly_fill(df2, 'Qty_L5', ly_start, ly_end, 52)\n",
    "# ly_fill(df2, 'Qty_LocType', ly_start, ly_end, 52)\n",
    "\n",
    "\n",
    "def Lag_Indices_MA_diff(Lag_df, Lag_Measure, Lag_Start, Lag_End, LY_Start,\n",
    "                        LY_End, LLY_Start, LLY_End):\n",
    "    Lag_Start_Loc = Lag_df.columns.get_loc(\n",
    "        str(Lag_Measure) + \"_Lag\" + str(Lag_Start))\n",
    "    Lag_End_Loc = Lag_df.columns.get_loc(\n",
    "        str(Lag_Measure) + \"_Lag\" + str(Lag_End)) + 1\n",
    "    Lag_df[Lag_Measure + \"_\" + str(Lag_Start) + \"_\" + str(\n",
    "        Lag_End) + \"_MA\"] = Lag_df.iloc[:, Lag_Start_Loc:Lag_End_Loc].mean(\n",
    "        axis=1, skipna=True)\n",
    "    LY_Start_Loc = Lag_df.columns.get_loc(\n",
    "        str(Lag_Measure) + \"_Lag\" + str(LY_Start))\n",
    "    LY_End_Loc = Lag_df.columns.get_loc(\n",
    "        str(Lag_Measure) + \"_Lag\" + str(LY_End)) + 1\n",
    "    Lag_df[Lag_Measure + \"_\" + str(LY_Start) + \"_\" + str(\n",
    "        LY_End) + \"_MA\"] = Lag_df.iloc[:, LY_Start_Loc:LY_End_Loc].mean(\n",
    "        axis=1, skipna=True)\n",
    "    LLY_Start_Loc = Lag_df.columns.get_loc(\n",
    "        str(Lag_Measure) + \"_Lag\" + str(LLY_Start))\n",
    "    LLY_End_Loc = Lag_df.columns.get_loc(\n",
    "        str(Lag_Measure) + \"_Lag\" + str(LLY_End)) + 1\n",
    "    Lag_df[Lag_Measure + \"_\" + str(LLY_Start) + \"_\" + str(\n",
    "        LLY_End) + \"_MA\"] = Lag_df.iloc[:, LLY_Start_Loc:LLY_End_Loc].mean(\n",
    "        axis=1, skipna=True)\n",
    "    Lag_df[Lag_Measure + \"_\" + str(Lag_Start) + \"_\" + str(\n",
    "        Lag_End) + \"_trend\"] = 0\n",
    "    for i in range(int(Lag_Start), int(Lag_End)):\n",
    "        Lag_df[Lag_Measure + \"_\" + str(i) + \"_\" + str(i + 1) + \"_diff\"] = \\\n",
    "            Lag_df[Lag_Measure + \"_Lag\" + str(i)] - Lag_df[\n",
    "                Lag_Measure + \"_Lag\" + str(i + 1)]\n",
    "        Lag_df[Lag_Measure + \"_\" + str(Lag_Start) + \"_\" + str(\n",
    "            Lag_End) + \"_trend\"] = Lag_df[Lag_Measure + \"_\" + str(\n",
    "            i) + \"_\" + str(i + 1) + \"_diff\"].fillna(0) + Lag_df[\n",
    "                                       Lag_Measure + \"_\" + str(\n",
    "                                           Lag_Start) + \"_\" + str(\n",
    "                                           Lag_End) + \"_trend\"]\n",
    "\n",
    "\n",
    "Lag_Indices_MA_diff(df2, \"Qty\", lag_start, lag_end, ly_start, ly_end,\n",
    "                    lly_start, lly_end)\n",
    "Lag_Indices_MA_diff(df2, \"Qty_Location\", lag_start, lag_end, ly_start,\n",
    "                    ly_end, lly_start, lly_end)\n",
    "Lag_Indices_MA_diff(df2, \"Qty_Item\", lag_start, lag_end, ly_start, ly_end,\n",
    "                    lly_start, lly_end)\n",
    "Lag_Indices_MA_diff(df2, \"Qty_L5\", lag_start, lag_end, ly_start, ly_end,\n",
    "                    lly_start, lly_end)\n",
    "Lag_Indices_MA_diff(df2, \"Qty_LocType\", lag_start, lag_end, ly_start,\n",
    "                    ly_end, lly_start, lly_end)\n",
    "\n",
    "# def seasonal_lag_feat(Lag_df, Lag_Measure, Lag_Start = 51, Lag_End = 53):\n",
    "#     #Backfill lag24 and lag 36 if needed\n",
    "\n",
    "#     Lag_Start_Loc = Lag_df.columns.get_loc(str(Lag_Measure)+\"_Lag\"+str(Lag_Start))\n",
    "#     Lag_End_Loc = Lag_df.columns.get_loc(str(Lag_Measure)+\"_Lag\"+str(Lag_End)) + 1\n",
    "#     Lag_df[Lag_Measure+\"_\"+str(Lag_Start)+\"_\"+str(Lag_End)+\"_diff\"] = \\\n",
    "#         Lag_df[Lag_Measure+\"_Lag\"+str(Lag_Start)] - Lag_df[Lag_Measure+\"_Lag\"+str(Lag_End)]\n",
    "#     Lag_df[Lag_Measure+\"_\"+str(Lag_Start)+\"_\"+str(Lag_End)+\"_perc\"] = \\\n",
    "#         Lag_df[Lag_Measure+\"_Lag\"+str(Lag_Start)] / Lag_df[Lag_Measure+\"_Lag\"+str(Lag_End)]\n",
    "#     Lag_df[Lag_Measure+\"_\"+str(Lag_Start)+\"_\"+str(Lag_End)+\"_mean\"] = \\\n",
    "#         Lag_df[[Lag_Measure+\"_Lag\"+str(Lag_Start), Lag_Measure+\"_Lag\"+str(Lag_End)]].mean(axis=1)\n",
    "\n",
    "\n",
    "# seasonal_lag_feat(df2,\"Qty\",51,53)\n",
    "# seasonal_lag_feat(df2,\"Qty_Location\",51,53)\n",
    "# seasonal_lag_feat(df2,\"Qty_Item\",51,53)\n",
    "# seasonal_lag_feat(df2,\"Qty_L5\",51,53)\n",
    "# seasonal_lag_feat(df2,\"Qty_LocType\",51,53)\n",
    "\n",
    "# seasonal_lag_feat(df2,\"Qty\",103,105)\n",
    "# seasonal_lag_feat(df2,\"Qty_Location\",103,105)\n",
    "# seasonal_lag_feat(df2,\"Qty_Item\",103,105)\n",
    "# seasonal_lag_feat(df2,\"Qty_L5\",103,105)\n",
    "# seasonal_lag_feat(df2,\"Qty_LocType\",103,105)\n",
    "\n",
    "\n",
    "####### Model Training, Tuning and Predicting #######\n",
    "\n",
    "\n",
    "df2 = pd.merge(df2, calendar_df, how='left', on='Week')\n",
    "\n",
    "df2['holidaytype'] = df2['holidaytype'].fillna(0)\n",
    "df2['holidayflag'] = df2['holidayflag'].fillna(0)\n",
    "\n",
    "holiday_col = [col for col in df2 if col.startswith('holiday')]\n",
    "\n",
    "for col in holiday_col:\n",
    "    df2[col + '_lead1'] = df2.groupby(['Item', 'Location',\n",
    "                                       'Sales Org'])[col].shift(-1)\n",
    "    df2[col + '_lead2'] = df2.groupby(['Item', 'Location',\n",
    "                                       'Sales Org'])[col].shift(-2)\n",
    "    df2[col + '_lead3'] = df2.groupby(['Item', 'Location',\n",
    "                                       'Sales Org'])[col].shift(-3)\n",
    "\n",
    "df2['holidaytype_lead1'] = df2['holidaytype_lead1'].fillna(0)\n",
    "df2['holidayflag_lead1'] = df2['holidayflag_lead1'].fillna(0)\n",
    "df2['holidaytype_lead2'] = df2['holidaytype_lead2'].fillna(0)\n",
    "df2['holidayflag_lead2'] = df2['holidayflag_lead2'].fillna(0)\n",
    "df2['holidaytype_lead3'] = df2['holidaytype_lead3'].fillna(0)\n",
    "df2['holidayflag_lead3'] = df2['holidayflag_lead3'].fillna(0)\n",
    "\n",
    "####Concat Item_Location#####\n",
    "df2['Item_Loc'] = df2['Item'] + '_' + df2['Location']\n",
    "\n",
    "###### Categorical feature encoding #######\n",
    "Item_Loc_be = pd.DataFrame(df2['Item_Loc'].drop_duplicates())\n",
    "encoder = ce.BinaryEncoder(cols='Item_Loc', return_df=True)\n",
    "Item_Loc_be = pd.concat([Item_Loc_be, encoder.fit_transform(Item_Loc_be)],\n",
    "                        axis=1)\n",
    "enc_cols = Item_Loc_be.iloc[:, 1:].columns.to_list()\n",
    "df2 = pd.merge(df2, Item_Loc_be, how='left', on='Item_Loc')\n",
    "\n",
    "encoder = ce.TargetEncoder(cols='Item_Loc', return_df=True)\n",
    "Item_Loc_te = pd.concat(\n",
    "    [df2['Item_Loc'], encoder.fit_transform(df2['Item_Loc'], df2['Qty'])],\n",
    "    axis=1)\n",
    "Item_Loc_te.columns = ['Item_Loc', 'Item_Loc_te']\n",
    "Item_Loc_te = pd.DataFrame(\n",
    "    Item_Loc_te[['Item_Loc', 'Item_Loc_te']].drop_duplicates())\n",
    "enc_cols = enc_cols + [Item_Loc_te.columns[1]]\n",
    "df2 = pd.merge(df2, Item_Loc_te, how='left', on='Item_Loc')\n",
    "\n",
    "###################Creating train df################################\n",
    "## Only filtering 36 months for training ##\n",
    "Train_Periods = 37\n",
    "Start_Time = hist_week - pd.DateOffset(months=Train_Periods)\n",
    "\n",
    "train_df = df2[df2['Week'] >= Start_Time]\n",
    "train_df['Lag'] = np.nan\n",
    "\n",
    "if env != \"local\":\n",
    "    logger.debug(\"train_df shape\")\n",
    "    logger.debug(train_df.shape)\n",
    "\n",
    "##########Generating sample weights#########\n",
    "\n",
    "###Generating row weights based on recency and seasonality###\n",
    "row_weights = train_df['Week']\n",
    "row_weights.drop_duplicates(inplace=True)\n",
    "row_weights = pd.DataFrame(row_weights).sort_values('Week').reset_index()\n",
    "row_weights['interval'] = hist_week + np.timedelta64(1, 'W') - row_weights[\n",
    "    'Week']\n",
    "row_weights['interval'] = (row_weights['interval'] / np.timedelta64(1,\n",
    "                                                                    'W')) - 1\n",
    "row_weights['weight'] = 1 * np.exp(-row_weights['interval'] * (0.0075))\n",
    "\n",
    "# Add seasonal weights ly\n",
    "seas_start = hist_week - np.timedelta64(7 * 54, 'D')\n",
    "seas_end = hist_week - np.timedelta64(7 * 50, 'D')\n",
    "seas_range = pd.date_range(start=seas_start, end=seas_end, freq='W-SUN')\n",
    "row_weights['weight'] = np.where(row_weights['Week'].isin(seas_range),\n",
    "                                 0.8, row_weights['weight'])\n",
    "\n",
    "# Add seasonal weights lly\n",
    "seas_start = hist_week - np.timedelta64(7 * 105, 'D')\n",
    "seas_end = hist_week - np.timedelta64(7 * 101, 'D')\n",
    "seas_range = pd.date_range(start=seas_start, end=seas_end, freq='W-SUN')\n",
    "row_weights['weight'] = np.where(row_weights['Week'].isin(seas_range),\n",
    "                                 0.7, row_weights['weight'])\n",
    "\n",
    "# Reduce weight during covid period\n",
    "seas_start = pd.to_datetime('2020-02-01')\n",
    "seas_end = pd.to_datetime('2020-07-01')\n",
    "seas_range = pd.date_range(start=seas_start, end=seas_end, freq='W-SUN')\n",
    "row_weights['weight'] = np.where(row_weights['Week'].isin(seas_range),\n",
    "                                 0.1, row_weights['weight'])\n",
    "\n",
    "# Reduce weight for 2020\n",
    "seas_start = pd.to_datetime('2020-09-01')\n",
    "seas_end = pd.to_datetime('2021-02-01')\n",
    "seas_range = pd.date_range(start=seas_start, end=seas_end, freq='W-SUN')\n",
    "row_weights['weight'] = np.where(row_weights['Week'].isin(seas_range),\n",
    "                                 0.3, row_weights['weight'])\n",
    "\n",
    "# train_df_orig = train_df_orig.merge(row_weights, how = 'left', on = 'Day')\n",
    "train_df = pd.merge(train_df.copy(), row_weights[['Week', 'weight']],\n",
    "                    how='left', on='Week')\n",
    "\n",
    "###joining segment df###\n",
    "\n",
    "train_df = pd.merge(train_df, segment_df, how='left', on=['Sales Org', 'Item'])\n",
    "\n",
    "if env != \"local\":\n",
    "    logger.debug(\"train_df shape after adding weights\")\n",
    "    logger.debug(train_df.shape)\n",
    "\n",
    "train_df['Month_Count'] = train_df['Month_Count'] + 21\n",
    "\n",
    "lead1list = ['Qty_Item_Lag103',\n",
    "'Qty_Item_Lag51',\n",
    "'Qty_L5_Lag103',\n",
    "'Qty_L5_Lag51',\n",
    "'Qty_Lag103',\n",
    "'Qty_Lag51',\n",
    "'Qty_LocType_Lag103',\n",
    "'Qty_LocType_Lag51',\n",
    "'Qty_Location_Lag103',\n",
    "'Qty_Location_Lag51',\n",
    "]\n",
    "\n",
    "for col in lead1list:\n",
    "    train_df[col+'_new'] = train_df.groupby(['Sales Org','Location', 'Item'])['Qty'].shift(-1)\n",
    "    train_df[col] = np.where(train_df[col].isna(),\n",
    "                             train_df[col + '_new'], train_df[col])\n",
    "    train_df.drop([col + '_new'],inplace=True,axis=1)\n",
    "\n",
    "lead43list = ['Qty_Item_Lag9',\n",
    "'Qty_L5_Lag9',\n",
    "'Qty_Lag9',\n",
    "'Qty_LocType_Lag9',\n",
    "'Qty_Location_Lag9',\n",
    "]\n",
    "\n",
    "for col in lead43list:\n",
    "    train_df[col+'_new'] = train_df.groupby(['Sales Org','Location', 'Item'])['Qty'].shift(-43)\n",
    "    train_df[col] = np.where(train_df[col].isna(),\n",
    "                             train_df[col + '_new'], train_df[col])\n",
    "    train_df.drop([col + '_new'], inplace=True, axis=1)\n",
    "\n",
    "lead44list = ['Qty_Item_Lag8',\n",
    "'Qty_L5_Lag8',\n",
    "'Qty_Lag8',\n",
    "'Qty_LocType_Lag8',\n",
    "'Qty_Location_Lag8',\n",
    "\n",
    "]\n",
    "\n",
    "for col in lead44list:\n",
    "    train_df[col+'_new'] = train_df.groupby(['Sales Org','Location', 'Item'])['Qty'].shift(-44)\n",
    "    train_df[col] = np.where(train_df[col].isna(),\n",
    "                             train_df[col + '_new'], train_df[col])\n",
    "    train_df.drop([col + '_new'], inplace=True, axis=1)\n",
    "\n",
    "lead45list = ['Qty_Item_Lag7',\n",
    "'Qty_L5_Lag7',\n",
    "'Qty_Lag7',\n",
    "'Qty_LocType_Lag7',\n",
    "'Qty_Location_Lag7',\n",
    "]\n",
    "\n",
    "for col in lead45list:\n",
    "    train_df[col+'_new'] = train_df.groupby(['Sales Org','Location', 'Item'])['Qty'].shift(-45)\n",
    "    train_df[col] = np.where(train_df[col].isna(),\n",
    "                             train_df[col + '_new'], train_df[col])\n",
    "    train_df.drop([col + '_new'], inplace=True, axis=1)\n",
    "\n",
    "lead46list = ['Qty_Item_Lag6',\n",
    "'Qty_L5_Lag6',\n",
    "'Qty_Lag6',\n",
    "'Qty_LocType_Lag6',\n",
    "'Qty_Location_Lag6',\n",
    "]\n",
    "\n",
    "for col in lead46list:\n",
    "    train_df[col+'_new'] = train_df.groupby(['Sales Org','Location', 'Item'])['Qty'].shift(-46)\n",
    "    train_df[col] = np.where(train_df[col].isna(),\n",
    "                             train_df[col + '_new'], train_df[col])\n",
    "    train_df.drop([col + '_new'], inplace=True, axis=1)\n",
    "\n",
    "lead47list = ['Qty_Item_Lag5',\n",
    "'Qty_L5_Lag5',\n",
    "'Qty_Lag5',\n",
    "'Qty_LocType_Lag5',\n",
    "'Qty_Location_Lag5',\n",
    "]\n",
    "\n",
    "for col in lead47list:\n",
    "    train_df[col+'_new'] = train_df.groupby(['Sales Org','Location', 'Item'])['Qty'].shift(-47)\n",
    "    train_df[col] = np.where(train_df[col].isna(),\n",
    "                             train_df[col + '_new'], train_df[col])\n",
    "    train_df.drop([col + '_new'], inplace=True, axis=1)\n",
    "\n",
    "lead48list = ['Qty_Item_Lag4',\n",
    "'Qty_L5_Lag4',\n",
    "'Qty_Lag4',\n",
    "'Qty_LocType_Lag4',\n",
    "'Qty_Location_Lag4',\n",
    "]\n",
    "\n",
    "for col in lead48list:\n",
    "    train_df[col+'_new'] = train_df.groupby(['Sales Org','Location', 'Item'])['Qty'].shift(-48)\n",
    "    train_df[col] = np.where(train_df[col].isna(),\n",
    "                             train_df[col + '_new'], train_df[col])\n",
    "    train_df.drop([col + '_new'], inplace=True, axis=1)\n",
    "\n",
    "lead0list = ['Qty_Item_Lag104',\n",
    "'Qty_Item_Lag105',\n",
    "'Qty_Item_Lag52',\n",
    "'Qty_Item_Lag53',\n",
    "'Qty_L5_Lag104',\n",
    "'Qty_L5_Lag105',\n",
    "'Qty_L5_Lag52',\n",
    "'Qty_L5_Lag53',\n",
    "'Qty_Lag104',\n",
    "'Qty_Lag105',\n",
    "'Qty_Lag52',\n",
    "'Qty_Lag53',\n",
    "'Qty_LocType_Lag104',\n",
    "'Qty_LocType_Lag105',\n",
    "'Qty_LocType_Lag52',\n",
    "'Qty_LocType_Lag53',\n",
    "'Qty_Location_Lag104',\n",
    "'Qty_Location_Lag105',\n",
    "'Qty_Location_Lag52',\n",
    "'Qty_Location_Lag53',\n",
    "]\n",
    "\n",
    "for col in lead0list:\n",
    "    train_df[col+'_new'] = train_df.groupby(['Sales Org','Location', 'Item'])['Qty'].shift(0)\n",
    "    train_df[col] = np.where(train_df[col].isna(), train_df[col+'_new'], train_df[col])\n",
    "    train_df.drop([col + '_new'], inplace=True, axis=1)\n",
    "\n",
    "\n",
    "###################Creating prediction df################################\n",
    "#### Create Test dataset ####\n",
    "Valid_SKUs = train_df[\n",
    "    train_df['Week'] > hist_week - np.timedelta64(31, 'D')]\n",
    "Valid_SKUs = Valid_SKUs.groupby(['Sales Org', 'Location', 'Item'])[\n",
    "    'Qty'].sum().reset_index()\n",
    "Valid_SKUs = Valid_SKUs[Valid_SKUs['Qty'] > 0]\n",
    "del Valid_SKUs['Qty']\n",
    "\n",
    "Fcst_End = hist_week + np.timedelta64(f_wk, 'W')\n",
    "Fcst_Start = hist_week + np.timedelta64(1, 'W')\n",
    "Forecast_Range = pd.date_range(start=Fcst_Start, end=Fcst_End, freq='W-SUN')\n",
    "Forecast_Range = pd.DataFrame(Forecast_Range)\n",
    "Forecast_Range['Lag'] = Forecast_Range.index - 1\n",
    "\n",
    "# Merging to create test df\n",
    "Valid_SKUs['key'] = 0\n",
    "Forecast_Range['key'] = 0\n",
    "test_df = Valid_SKUs.merge(Forecast_Range, how='outer', on='key')\n",
    "del test_df['key']\n",
    "test_df.columns = ['Sales Org', 'Location', 'Item', 'Week', 'Lag']\n",
    "test_df['Item'] = test_df['Item'].astype('str')\n",
    "test_df['Location'] = test_df['Location'].astype('str')\n",
    "test_df['Qty'] = np.nan\n",
    "\n",
    "if env != \"local\":\n",
    "    logger.debug(\"test_df shape after creating test dataset\")\n",
    "    logger.debug(test_df.shape)\n",
    "\n",
    "### Appending with train df ###\n",
    "\n",
    "test_df = test_df.append(train_df[test_df.columns]).sort_values(\n",
    "    ['Sales Org', 'Location', 'Item', 'Week'], ascending=True)\n",
    "\n",
    "if env != \"local\":\n",
    "    logger.debug(\"test_df shape after appending with train_df\")\n",
    "    logger.debug(test_df.shape)\n",
    "\n",
    "##Repeat dataframe creation steps##\n",
    "test_df = pd.merge(test_df, prod_df, how='left', on='Item')\n",
    "test_df = pd.merge(test_df, loc_df, how='left', on='Location')\n",
    "###Creating Item_Loc Key###\n",
    "test_df['Item_Loc'] = test_df['Item'] + '_' + test_df['Location']\n",
    "\n",
    "test_df = time_features(test_df)\n",
    "test_df = agg_features(test_df)\n",
    "test_df = lag_features_fn(test_df, col_index, col_time, col_shift,\n",
    "                          lag_start, lag_end, ly_start, ly_end, lly_start,\n",
    "                          lly_end)\n",
    "\n",
    "# ly_fill(test_df, 'Qty', ly_start, ly_end, 52)\n",
    "# ly_fill(test_df, 'Qty_Item', ly_start, ly_end, 52)\n",
    "# ly_fill(test_df, 'Qty_Location', ly_start, ly_end, 52)\n",
    "# ly_fill(test_df, 'Qty_L5', ly_start, ly_end, 52)\n",
    "# ly_fill(test_df, 'Qty_LocType', ly_start, ly_end, 52)\n",
    "\n",
    "\n",
    "Lag_Indices_MA_diff(test_df, \"Qty\", lag_start, lag_end, ly_start, ly_end,\n",
    "                    lly_start, lly_end)\n",
    "Lag_Indices_MA_diff(test_df, \"Qty_Location\", lag_start, lag_end, ly_start,\n",
    "                    ly_end, lly_start, lly_end)\n",
    "Lag_Indices_MA_diff(test_df, \"Qty_Item\", lag_start, lag_end, ly_start,\n",
    "                    ly_end, lly_start, lly_end)\n",
    "Lag_Indices_MA_diff(test_df, \"Qty_L5\", lag_start, lag_end, ly_start, ly_end,\n",
    "                    lly_start, lly_end)\n",
    "Lag_Indices_MA_diff(test_df, \"Qty_LocType\", lag_start, lag_end, ly_start,\n",
    "                    ly_end, lly_start, lly_end)\n",
    "\n",
    "# seasonal_lag_feat(df2,\"Qty\",51,53)\n",
    "# seasonal_lag_feat(df2,\"Qty_Location\",51,53)\n",
    "# seasonal_lag_feat(df2,\"Qty_Item\",51,53)\n",
    "# seasonal_lag_feat(df2,\"Qty_L5\",51,53)\n",
    "# seasonal_lag_feat(df2,\"Qty_LocType\",51,53)\n",
    "\n",
    "# seasonal_lag_feat(df2,\"Qty\",103,105)\n",
    "# seasonal_lag_feat(df2,\"Qty_Location\",103,105)\n",
    "# seasonal_lag_feat(df2,\"Qty_Item\",103,105)\n",
    "# seasonal_lag_feat(df2,\"Qty_L5\",103,105)\n",
    "# seasonal_lag_feat(df2,\"Qty_LocType\",103,105)\n",
    "\n",
    "\n",
    "test_df = pd.merge(test_df, calendar_df, how='left', on='Week')\n",
    "\n",
    "test_df['holidaytype'] = test_df['holidaytype'].fillna(0)\n",
    "test_df['holidayflag'] = test_df['holidayflag'].fillna(0)\n",
    "\n",
    "holiday_col = [col for col in test_df if col.startswith('holiday')]\n",
    "\n",
    "for col in holiday_col:\n",
    "    test_df[col + '_lead1'] = test_df.groupby(['Item', 'Location',\n",
    "                                               'Sales Org'])[col].shift(-1)\n",
    "    test_df[col + '_lead2'] = test_df.groupby(['Item', 'Location',\n",
    "                                               'Sales Org'])[col].shift(-2)\n",
    "    test_df[col + '_lead3'] = test_df.groupby(['Item', 'Location',\n",
    "                                               'Sales Org'])[col].shift(-3)\n",
    "\n",
    "test_df['holidaytype_lead1'] = test_df['holidaytype_lead1'].fillna(0)\n",
    "test_df['holidayflag_lead1'] = test_df['holidayflag_lead1'].fillna(0)\n",
    "test_df['holidaytype_lead2'] = test_df['holidaytype_lead2'].fillna(0)\n",
    "test_df['holidayflag_lead2'] = test_df['holidayflag_lead2'].fillna(0)\n",
    "test_df['holidaytype_lead3'] = test_df['holidaytype_lead3'].fillna(0)\n",
    "test_df['holidayflag_lead3'] = test_df['holidayflag_lead3'].fillna(0)\n",
    "\n",
    "###joining segment df to test_df###\n",
    "\n",
    "test_df = pd.merge(test_df, segment_df, how='left', on=['Sales Org', 'Item'])\n",
    "\n",
    "test_df = pd.merge(test_df, Item_Loc_be, how='left', on='Item_Loc')\n",
    "test_df = pd.merge(test_df, Item_Loc_te, how='left', on='Item_Loc')\n",
    "\n",
    "##Filtering Lag1 Month##\n",
    "test_df_lr = test_df.copy()\n",
    "test_df = test_df[test_df['Lag'] == 2]\n",
    "\n",
    "\n",
    "###################Training####################\n",
    "#####Feature_Imp_cols#####\n",
    "def get_feature_imp(algo, algo_name):\n",
    "    algo = lgb\n",
    "    importance = algo.feature_importances_.round(4)\n",
    "    names = np.array(feature_cols)\n",
    "    Feature_Imp = pd.DataFrame(data=np.column_stack((names, importance)\n",
    "                                                    ),\n",
    "                               columns=['names', 'importance'])\n",
    "    Feature_Imp.columns = ['Feature', 'Feature_Importance']\n",
    "    Feature_Imp['Week'] = hist_week\n",
    "    Feature_Imp = Feature_Imp[['Week', 'Feature', 'Feature_Importance']]\n",
    "    Feature_Imp = Feature_Imp.rename(\n",
    "        columns={'Feature_Importance': algo_name})\n",
    "    return (Feature_Imp)\n",
    "\n",
    "\n",
    "########Model1: Predicting the lag 2 week###############\n",
    "\n",
    "ts_cols = ['Year_Nominal', 'Year_Num',\n",
    "           'Quarter_Num', 'Quarter_Sin', 'Quarter_Cos', 'Month_Num',\n",
    "           'Month_Sin',\n",
    "           'Month_Cos', 'Month_Count', 'Week_Num', 'Week_Sin', 'Week_Cos',\n",
    "           'WOM_Num', 'Year_End',\n",
    "           'Qty_Lag4', 'Qty_Lag5', 'Qty_Lag6', 'Qty_Lag7', 'Qty_Lag8',\n",
    "           'Qty_Lag9',\n",
    "           'Qty_Lag51', 'Qty_Lag52', 'Qty_Lag53', 'Qty_Lag104',\n",
    "           'Qty_Lag105',\n",
    "           'Qty_4_9_MA', 'Qty_51_53_MA', 'Qty_4_9_trend', 'Qty_4_5_diff',\n",
    "           'Qty_5_6_diff', 'Qty_6_7_diff',\n",
    "           'Qty_Item_Lag4', 'Qty_Item_Lag5', 'Qty_Item_Lag6',\n",
    "           'Qty_Item_Lag52',\n",
    "           'Qty_Item_4_9_MA', 'Qty_Item_51_53_MA', 'Qty_Item_4_9_trend',\n",
    "           'Qty_Item_4_5_diff',\n",
    "           'Qty_Location_Lag4', 'Qty_Location_Lag5', 'Qty_Location_Lag6',\n",
    "           'Qty_Location_Lag52',\n",
    "           'Qty_Location_4_9_MA', 'Qty_Location_51_53_MA',\n",
    "           'Qty_Location_4_9_trend', 'Qty_Location_4_5_diff',\n",
    "           'Qty_L5_Lag4', 'Qty_L5_Lag5', 'Qty_L5_Lag6', 'Qty_L5_Lag52',\n",
    "           'Qty_L5_4_9_MA', 'Qty_L5_51_53_MA', 'Qty_L5_4_9_trend',\n",
    "           'Qty_L5_4_5_diff', 'holidayflag_lead1', 'holidayflag_lead2',\n",
    "           'holidayflag_lead3']\n",
    "\n",
    "cat_cols = ['Item', 'L3', 'L5', 'LocType', 'Region', 'holidaytype_lead1',\n",
    "            'holidaytype_lead2', 'holidaytype_lead3', 'Product Segment']\n",
    "\n",
    "feature_cols = ts_cols + cat_cols\n",
    "\n",
    "train_df[cat_cols] = train_df[cat_cols].astype(\"category\")\n",
    "test_df[cat_cols] = test_df[cat_cols].astype(\"category\")\n",
    "\n",
    "####Creating copies of original test and train df###\n",
    "# train_df_orig = train_df.copy()\n",
    "# test_df_orig = test_df.copy()\n",
    "feature_cols_orig = feature_cols\n",
    "\n",
    "######################Slicing the model######################\n",
    "# #By Segment Group\n",
    "\n",
    "# train_df_orig['Slice'] = train_df_orig['L6']\n",
    "# test_df_orig['Slice'] = test_df_orig['L6']\n",
    "\n",
    "# train_df_orig['Slice'] = train_df_orig['L1'].astype(str) + \"_\" + train_df_orig['Group'].astype(str)\n",
    "# test_df_orig['Slice'] = test_df_orig['L1'].astype(str) + \"_\" + test_df_orig['Group'].astype(str)\n",
    "\n",
    "# train_df_orig['Slice'] = train_df_orig['Location'].astype(str) + \"_\" + train_df_orig['Group'].astype(str)\n",
    "# test_df_orig['Slice'] = test_df_orig['Location'].astype(str) + \"_\" + test_df_orig['Group'].astype(str)\n",
    "\n",
    "# iter_list = train_df_orig['Slice'].unique()\n",
    "# iter_count = iter_list.size + 1\n",
    "\n",
    "# for j in range(1, iter_count):\n",
    "#     # j=1\n",
    "try:\n",
    "    # if env == \"local\":\n",
    "    #     print(str(j) + \" out of \" + str(iter_count - 1) + \" : \" + iter_list[\n",
    "    #     j - 1])\n",
    "    # else:\n",
    "    #     logger.debug(str(j) + \" out of \" + str(iter_count - 1) + \" : \" +\n",
    "    #                  iter_list[\n",
    "    #                      j - 1])\n",
    "\n",
    "    # train_df = train_df_orig.copy()\n",
    "\n",
    "    if env != \"local\":\n",
    "        logger.debug(\"model run starting\")\n",
    "    else:\n",
    "        print(\"model run starting\")\n",
    "\n",
    "    ###XGB RF##\n",
    "    ##Select feature set##\n",
    "    feature_cols = ts_cols\n",
    "    ## Creating the Training matrix with all feature columns##\n",
    "    X = train_df.loc[:, feature_cols]\n",
    "    ## Creating response vector\n",
    "    y = train_df['Qty']\n",
    "    # xgb_rf = XGBRFRegressor(random_state=42)\n",
    "    if env != \"local\":\n",
    "        logger.debug(\"xgb_rf model run starting\")\n",
    "    else:\n",
    "        print(\"xgb_rf model run starting\")\n",
    "    xgb_rf = XGBRFRegressor(n_estimators=500, random_state=42)\n",
    "    # fit the model\n",
    "    start_time = time.time()\n",
    "    xgb_rf.fit(X, y, sample_weight=train_df.weight)\n",
    "\n",
    "    if env == \"local\":\n",
    "        print(\"xgb_rf --- %s seconds ---\" % (time.time() - start_time))\n",
    "    else:\n",
    "        logger.debug(\"xgb_rf --- %s seconds ---\" % (time.time() -\n",
    "                                                    start_time))\n",
    "\n",
    "    # Feature_Imp_all = get_feature_imp(xgb_rf,'xgb_rf')\n",
    "    # Feature_Imp_all = pd.merge(Feature_Imp_all, get_feature_imp(xgb_rf,'xgb_rf'),\n",
    "    #                         how = 'left', on = ['Time','Feature'])\n",
    "\n",
    "    ###xgb##\n",
    "    if env != \"local\":\n",
    "        logger.debug(\"xgb model run starting\")\n",
    "    else:\n",
    "        print(\"xgb model run starting\")\n",
    "    xgb = XGBRegressor(n_estimators=500, learning_rate=0.1,\n",
    "                       random_state=42)\n",
    "    # fit the model\n",
    "    start_time = time.time()\n",
    "    xgb.fit(X, y, sample_weight=train_df.weight)\n",
    "\n",
    "    if env == \"local\":\n",
    "        print(\"xgb --- %s seconds ---\" % (time.time() - start_time))\n",
    "    else:\n",
    "        logger.debug(\"xgb --- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "    # Feature_Imp_all = pd.merge(Feature_Imp_all, get_feature_imp(xgb,'xgb'),\n",
    "    #                   how = 'left', on = ['Week','Feature'])\n",
    "\n",
    "    ###LGBM##\n",
    "    ##Select feature set##\n",
    "    feature_cols = feature_cols_orig\n",
    "    ## Creating the Training matrix with all feature columns##\n",
    "    train_df = train_df.dropna(subset=['Product Segment'])\n",
    "\n",
    "    X = train_df.loc[:, feature_cols]\n",
    "    ## Creating response vector\n",
    "    y = train_df['Qty']\n",
    "    if env != \"local\":\n",
    "        logger.debug(\"lgbm model run starting\")\n",
    "    else:\n",
    "        print(\"lgbm model run starting\")\n",
    "    lgb = LGBMRegressor(n_estimators=2000, learning_rate=0.005,\n",
    "                        max_depth=10, num_leaves=int((2 ** 10) / 2),\n",
    "                        max_bin=1000,\n",
    "                        random_state=42)\n",
    "    # lgb = LGBMRegressor(random_state=42, verbose = -1)\n",
    "    # fit the model\n",
    "    start_time = time.time()\n",
    "    lgb.fit(X, y, sample_weight=train_df.weight)\n",
    "\n",
    "    if env == \"local\":\n",
    "        print(\"lgb --- %s seconds ---\" % (time.time() - start_time))\n",
    "    else:\n",
    "        logger.debug(\"lgb --- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "    # Feature_Imp_all = pd.merge(Feature_Imp_all, get_feature_imp(lgb,'lgb'),\n",
    "    #                   how = 'left', on = ['Week','Feature'])\n",
    "\n",
    "    ###Catboost##\n",
    "    # cb = CatBoostRegressor(n_estimators=5000, learning_rate = 0.005,\n",
    "    #                        # cat_features = cat_cols, one_hot_max_size = 16,\n",
    "    #                        random_state=42, verbose=0)\n",
    "    if env != \"local\":\n",
    "        logger.debug(\"catb model run starting\")\n",
    "    else:\n",
    "        print(\"catb model run starting\")\n",
    "    cb = CatBoostRegressor(n_estimators=1000, learning_rate=0.01,\n",
    "                           cat_features=cat_cols, one_hot_max_size=16,\n",
    "                           random_state=42, verbose=0)\n",
    "    # fit the model\n",
    "    start_time = time.time()\n",
    "    cb.fit(X, y, sample_weight=train_df.weight)\n",
    "\n",
    "    if env == \"local\":\n",
    "        print(\"cb --- %s seconds ---\" % (time.time() - start_time))\n",
    "    else:\n",
    "        logger.debug(\"cb --- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "    # Feature_Imp_all = pd.merge(Feature_Imp_all, get_feature_imp(cb,'cb'),\n",
    "    #                   how = 'left', on = ['Time','Feature'])\n",
    "\n",
    "    ########################Predicting########################\n",
    "    # test_df = test_df_orig.copy()\n",
    "\n",
    "    #####Prediction#####\n",
    "    feature_cols = ts_cols\n",
    "    y_pred = xgb_rf.predict(test_df.loc[:, feature_cols])\n",
    "    test_df['xgb_rf'] = y_pred\n",
    "    test_df['xgb_rf'][test_df['xgb_rf'] < 0] = 0\n",
    "\n",
    "    y_pred = xgb.predict(test_df.loc[:, feature_cols])\n",
    "    test_df['xgb'] = y_pred\n",
    "    test_df['xgb'][test_df['xgb'] < 0] = 0\n",
    "\n",
    "    feature_cols = feature_cols_orig\n",
    "    y_pred = lgb.predict(test_df.loc[:, feature_cols])\n",
    "    test_df['lgb'] = y_pred\n",
    "    test_df['lgb'][test_df['lgb'] < 0] = 0\n",
    "\n",
    "    y_pred = cb.predict(test_df.loc[:, feature_cols])\n",
    "    test_df['cb'] = y_pred\n",
    "    test_df['cb'][test_df['cb'] < 0] = 0\n",
    "\n",
    "    # # Appending Feature Imp\n",
    "    # if j == 1:\n",
    "    #     Feature_Imp_df = Feature_Imp_all\n",
    "    # else:\n",
    "    #     Feature_Imp_df = Feature_Imp_df.append(Feature_Imp_all)\n",
    "\n",
    "    # Appending test_df\n",
    "    # if j == 1:\n",
    "    #     test_df_all = test_df\n",
    "    # else:\n",
    "    #     test_df_all = test_df_all.append(test_df)\n",
    "\n",
    "    # test_df_all = test_df.copy()\n",
    "\n",
    "    if env == \"local\":\n",
    "        print(\"test_df\")\n",
    "        print(test_df.head(10))\n",
    "    else:\n",
    "        logger.debug(\"test_df\")\n",
    "        logger.debug(test_df.head(10))\n",
    "        logger.debug(\"test df \")\n",
    "        logger.debug(test_df.dtypes)\n",
    "        import datetime\n",
    "\n",
    "        t = datetime.date(2022, 11, 6)\n",
    "\n",
    "        logger.debug(test_df[(test_df['Week'] == t) & (\n",
    "                    test_df['Item'] == '000000447000198300') & (\n",
    "                                         test_df['Location'] == '0457')])\n",
    "\n",
    "except:\n",
    "    pass\n",
    "\n",
    "####Outputting####\n",
    "output_df_l2 = test_df[['Sales Org', 'Location', 'Item', 'Week', 'Qty',\n",
    "                        'xgb_rf', 'xgb', 'lgb', 'cb']]\n",
    "\n",
    "if env != \"local\":\n",
    "    logger.debug(\"Shape of output df l2 before final format\")\n",
    "    logger.debug(output_df_l2.shape)\n",
    "    logger.debug(\"output df l2 before final format\")\n",
    "    logger.debug(output_df_l2.head())\n",
    "    logger.debug(output_df_l2.dtypes)\n",
    "    logger.debug(\"intersection op df l2\")\n",
    "    import datetime\n",
    "\n",
    "    t = datetime.date(2022, 11, 6)\n",
    "    logger.debug(output_df_l2[(output_df_l2['Week'] == t) & (\n",
    "                output_df_l2['Item'] == '000000447000198300') & (\n",
    "                                          output_df_l2['Location'] == '0457')])\n",
    "\n",
    "########Model2: Predicting long range (lr) forecast###############\n",
    "\n",
    "ts_cols = ['Year_Nominal', 'Year_Num',\n",
    "           'Quarter_Num', 'Quarter_Sin', 'Quarter_Cos', 'Month_Num',\n",
    "           'Month_Sin',\n",
    "           'Month_Cos', 'Month_Count', 'Week_Num', 'Week_Sin', 'Week_Cos',\n",
    "           'WOM_Num', 'Year_End',\n",
    "           'holidayflag_lead1', 'holidayflag_lead2',\n",
    "           'holidayflag_lead3']\n",
    "\n",
    "cat_cols = ['Item_Loc', 'L3', 'LocType', 'holidaytype_lead1',\n",
    "            'holidaytype_lead2', 'holidaytype_lead3', 'Product Segment']\n",
    "\n",
    "feature_cols = ts_cols + enc_cols + cat_cols\n",
    "feature_cols_orig = feature_cols\n",
    "\n",
    "train_df[cat_cols] = train_df[cat_cols].astype(\"category\")\n",
    "test_df_lr[cat_cols] = test_df_lr[cat_cols].astype(\"category\")\n",
    "\n",
    "# Copy of test dataframe\n",
    "# test_df_lr_orig = test_df_lr.copy()\n",
    "\n",
    "######################Slicing the model######################\n",
    "# #By Segment Group\n",
    "\n",
    "# train_df_orig['Slice'] = train_df_orig['L6']\n",
    "# test_df_orig['Slice'] = test_df_orig['L6']\n",
    "\n",
    "# train_df_orig['Slice'] = train_df_orig['L1'].astype(str) + \"_\" + train_df_orig['Group'].astype(str)\n",
    "# test_df_orig['Slice'] = test_df_orig['L1'].astype(str) + \"_\" + test_df_orig['Group'].astype(str)\n",
    "\n",
    "# train_df_orig['Slice'] = train_df_orig['Location'].astype(str) + \"_\" + train_df_orig['Group'].astype(str)\n",
    "# test_df_orig['Slice'] = test_df_orig['Location'].astype(str) + \"_\" + test_df_orig['Group'].astype(str)\n",
    "\n",
    "# iter_list = train_df_orig['Slice'].unique()\n",
    "# iter_count = iter_list.size + 1\n",
    "\n",
    "# for j in range(1, iter_count):\n",
    "#     # j=1\n",
    "try:\n",
    "    # if env == \"local\":\n",
    "    #     print(str(j) + \" out of \" + str(iter_count - 1) + \" : \" + iter_list[\n",
    "    #     j - 1])\n",
    "    # else:\n",
    "    #     logger.debug(str(j) + \" out of \" + str(iter_count - 1) + \" : \" +\n",
    "    #                  iter_list[\n",
    "    #                      j - 1])\n",
    "\n",
    "    # train_df = train_df_orig.copy()\n",
    "    if env != \"local\":\n",
    "        logger.debug(\"model run starting fwd\")\n",
    "    else:\n",
    "        print(\"model run starting fwd\")\n",
    "\n",
    "    ###XGB RF##\n",
    "    ##Select feature set##\n",
    "    enc_cols = ['Item_Loc_te']\n",
    "    feature_cols = ts_cols + enc_cols\n",
    "    ## Creating the Training matrix with all feature columns##\n",
    "    X = train_df.loc[:, feature_cols]\n",
    "    ## Creating response vector\n",
    "    y = train_df['Qty']\n",
    "    # xgb_rf = XGBRFRegressor(random_state=42)\n",
    "    if env != \"local\":\n",
    "        logger.debug(\"xgb_rf model run starting\")\n",
    "    else:\n",
    "        print(\"xgb_rf model run starting\")\n",
    "    xgb_rf = XGBRFRegressor(n_estimators=1000, random_state=42)\n",
    "    # fit the model\n",
    "    start_time = time.time()\n",
    "    xgb_rf.fit(X, y, sample_weight=train_df.weight)\n",
    "\n",
    "    if env == \"local\":\n",
    "        print(\"xgb_rf --- %s seconds ---\" % (time.time() - start_time))\n",
    "    else:\n",
    "        logger.debug(\"xgb_rf --- %s seconds ---\" % (time.time() -\n",
    "                                                    start_time))\n",
    "\n",
    "    # Feature_Imp_all = get_feature_imp(xgb_rf,'xgb_rf')\n",
    "    # Feature_Imp_all = pd.merge(Feature_Imp_all, get_feature_imp(xgb_rf,'xgb_rf'),\n",
    "    #                         how = 'left', on = ['Time','Feature'])\n",
    "\n",
    "    ###xgb##\n",
    "    if env != \"local\":\n",
    "        logger.debug(\"xgb model run starting\")\n",
    "    else:\n",
    "        print(\"xgb model run starting\")\n",
    "    xgb = XGBRegressor(n_estimators=1000, learning_rate=0.01,\n",
    "                       random_state=42)\n",
    "    # fit the model\n",
    "    start_time = time.time()\n",
    "    xgb.fit(X, y, sample_weight=train_df.weight)\n",
    "\n",
    "    if env == \"local\":\n",
    "        print(\"xgb --- %s seconds ---\" % (time.time() - start_time))\n",
    "    else:\n",
    "        logger.debug(\"xgb --- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "    # Feature_Imp_all = pd.merge(Feature_Imp_all, get_feature_imp(xgb,'xgb'),\n",
    "    #                   how = 'left', on = ['Week','Feature'])\n",
    "\n",
    "    ###LGBM##\n",
    "    ##Select feature set##\n",
    "    feature_cols = feature_cols_orig\n",
    "    ## Creating the Training matrix with all feature columns##\n",
    "    train_df = train_df.dropna(subset=['Product Segment'])\n",
    "\n",
    "    X = train_df.loc[:, feature_cols]\n",
    "    ## Creating response vector\n",
    "    y = train_df['Qty']\n",
    "    if env != \"local\":\n",
    "        logger.debug(\"lgb model run starting\")\n",
    "    else:\n",
    "        print(\"lgb model run starting\")\n",
    "    lgb = LGBMRegressor(n_estimators=2000, learning_rate=0.005,\n",
    "                        max_depth=10, num_leaves=int((2 ** 10) / 2),\n",
    "                        max_bin=1000,\n",
    "                        random_state=42)\n",
    "    # lgb = LGBMRegressor(random_state=42, verbose = -1)\n",
    "    # fit the model\n",
    "    start_time = time.time()\n",
    "    lgb.fit(X, y, sample_weight=train_df.weight)\n",
    "\n",
    "    if env == \"local\":\n",
    "        print(\"lgb --- %s seconds ---\" % (time.time() - start_time))\n",
    "    else:\n",
    "        logger.debug(\"lgb --- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "    # Feature_Imp_all = pd.merge(Feature_Imp_all, get_feature_imp(lgb,'lgb'),\n",
    "    #                   how = 'left', on = ['Week','Feature'])\n",
    "\n",
    "    ###Catboost##\n",
    "    # cb = CatBoostRegressor(n_estimators=5000, learning_rate = 0.005,\n",
    "    #                        # cat_features = cat_cols, one_hot_max_size = 16,\n",
    "    #                        random_state=42, verbose=0)\n",
    "    if env != \"local\":\n",
    "        logger.debug(\"catb model run starting\")\n",
    "    else:\n",
    "        print(\"catb model run starting\")\n",
    "    cb = CatBoostRegressor(n_estimators=1000, learning_rate=0.01,\n",
    "                           cat_features=cat_cols, one_hot_max_size=16,\n",
    "                           random_state=42, verbose=0)\n",
    "    # fit the model\n",
    "    start_time = time.time()\n",
    "    cb.fit(X, y, sample_weight=train_df.weight)\n",
    "\n",
    "    if env == \"local\":\n",
    "        print(\"cb --- %s seconds ---\" % (time.time() - start_time))\n",
    "    else:\n",
    "        logger.debug(\"cb --- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "    # Feature_Imp_all = pd.merge(Feature_Imp_all, get_feature_imp(cb,'cb'),\n",
    "    #                   how = 'left', on = ['Time','Feature'])\n",
    "\n",
    "    ########################Predicting########################\n",
    "    # test_df = test_df_lr.copy()\n",
    "\n",
    "    #####Prediction#####\n",
    "    feature_cols = ts_cols + enc_cols\n",
    "    y_pred = xgb_rf.predict(test_df_lr.loc[:, feature_cols])\n",
    "    test_df_lr['xgb_rf'] = y_pred\n",
    "    test_df_lr['xgb_rf'][test_df_lr['xgb_rf'] < 0] = 0\n",
    "\n",
    "    y_pred = xgb.predict(test_df_lr.loc[:, feature_cols])\n",
    "    test_df_lr['xgb'] = y_pred\n",
    "    test_df_lr['xgb'][test_df_lr['xgb'] < 0] = 0\n",
    "\n",
    "    feature_cols = feature_cols_orig\n",
    "    y_pred = lgb.predict(test_df_lr.loc[:, feature_cols])\n",
    "    test_df_lr['lgb'] = y_pred\n",
    "    test_df_lr['lgb'][test_df_lr['lgb'] < 0] = 0\n",
    "\n",
    "    y_pred = cb.predict(test_df_lr.loc[:, feature_cols])\n",
    "    test_df_lr['cb'] = y_pred\n",
    "    test_df_lr['cb'][test_df_lr['cb'] < 0] = 0\n",
    "\n",
    "    # # Appending Feature Imp\n",
    "    # if j == 1:\n",
    "    #     Feature_Imp_df = Feature_Imp_all\n",
    "    # else:\n",
    "    #     Feature_Imp_df = Feature_Imp_df.append(Feature_Imp_all)\n",
    "\n",
    "    # Appending test_df_lr\n",
    "    # if j == 1:\n",
    "    #     test_df_lr_all = test_df_lr\n",
    "    # else:\n",
    "    #     test_df_lr_all = test_df_lr_all.append(test_df_lr)\n",
    "\n",
    "    # test_df_lr_all = test_df_lr.copy()\n",
    "\n",
    "    if env == \"local\":\n",
    "        print(\"test_df_lr\")\n",
    "        print(test_df_lr.head(10))\n",
    "    else:\n",
    "        logger.debug(\"test_df_lr\")\n",
    "        logger.debug(test_df_lr.head(10))\n",
    "        logger.debug(test_df_lr.dtypes)\n",
    "        logger.debug(\"test_df_lr\")\n",
    "        import datetime\n",
    "\n",
    "        t = datetime.date(2022, 11, 6)\n",
    "        logger.debug(test_df_lr[(test_df_lr['Week'] == t) & (\n",
    "                    test_df_lr['Item'] == '000000447000198300') & (\n",
    "                                            test_df_lr['Location'] == '0457')])\n",
    "\n",
    "except:\n",
    "    pass\n",
    "\n",
    "####Outputting####\n",
    "output_df = test_df_lr[['Sales Org', 'Location', 'Item', 'Week', 'Qty',\n",
    "                        'xgb_rf', 'xgb', 'lgb', 'cb']]\n",
    "\n",
    "####Merging lag 2 week with lr output####\n",
    "output_df_l2.columns = ['Sales Org', 'Location', 'Item', 'Week', 'Qty',\n",
    "                        'xgb_rf_l2', 'xgb_l2', 'lgb_l2', 'cb_l2']\n",
    "\n",
    "output_df = pd.merge(output_df, output_df_l2, how='left',\n",
    "                     on=['Sales Org', 'Location', 'Item', 'Week'])\n",
    "\n",
    "output_df['xgb_rf'] = np.where(output_df['xgb_rf_l2'].notna(),\n",
    "                               output_df['xgb_rf_l2'], output_df['xgb_rf'])\n",
    "output_df['xgb'] = np.where(output_df['xgb_l2'].notna(), output_df['xgb_l2'],\n",
    "                            output_df['xgb'])\n",
    "output_df['lgb'] = np.where(output_df['lgb_l2'].notna(), output_df['lgb_l2'],\n",
    "                            output_df['lgb'])\n",
    "output_df['cb'] = np.where(output_df['cb_l2'].notna(), output_df['cb_l2'],\n",
    "                           output_df['cb'])\n",
    "\n",
    "if env != \"local\":\n",
    "    logger.debug(\n",
    "        \"Shape of output df before final format and after merge with output df l2\")\n",
    "    logger.debug(output_df.shape)\n",
    "    logger.debug(\n",
    "        \"output df before final format and after merge with output df l2\")\n",
    "    logger.debug(output_df.head())\n",
    "    logger.debug(output_df.dtypes)\n",
    "    import datetime\n",
    "\n",
    "    t = datetime.date(2022, 11, 6)\n",
    "    logger.debug(output_df[(output_df['Week'] == t) & (\n",
    "                output_df['Item'] == '000000447000198300') & (\n",
    "                                       output_df['Location'] == '0457')])\n",
    "\n",
    "# ###Disaggregation Logic based on last 6 weeks###\n",
    "# nw = 6\n",
    "# ll_df = tran_df[(tran_df['Week'] > hist_week - np.timedelta64(nw, 'W')) & (\n",
    "#             tran_df['Week'] <= hist_week)]\n",
    "# ll_df = ll_df.groupby(['Sales Org', 'Location', 'Customer',\n",
    "#                        'Item', 'EPM'])['Qty'].sum().reset_index()\n",
    "# ll_df.rename(columns={'Qty': 'Qty_ll'}, inplace=True)\n",
    "#\n",
    "# hl_df = tran_df[(tran_df['Week'] > hist_week - np.timedelta64(nw, 'W')) & (\n",
    "#             tran_df['Week'] <= hist_week)]\n",
    "#\n",
    "# hl_df = hl_df.groupby(['Sales Org', 'Location',\n",
    "#                        'Item', ])['Qty'].sum().reset_index()\n",
    "# hl_df.rename(columns={'Qty': 'Qty_hl'}, inplace=True)\n",
    "#\n",
    "# hl_df = pd.merge(hl_df, ll_df, how='left',\n",
    "#                  on=['Sales Org', 'Location', 'Item'])\n",
    "# hl_df['perc'] = hl_df['Qty_ll'] / hl_df['Qty_hl']\n",
    "# hl_df['perc'] = hl_df['perc'].fillna(0)\n",
    "#\n",
    "# ###Disaagreagating the forecast###\n",
    "# # temp = output_df\n",
    "# output_df = pd.merge(output_df, hl_df, how='left',\n",
    "#                      on=['Sales Org', 'Location', 'Item'])\n",
    "# output_df['MLL1'] = output_df['xgb_rf'] * output_df['perc']\n",
    "# output_df['MLL2'] = output_df['xgb'] * output_df['perc']\n",
    "# output_df['MLL3'] = output_df['lgb'] * output_df['perc']\n",
    "# output_df['MLL4'] = output_df['cb'] * output_df['perc']\n",
    "# output_df['MLL5'] = output_df[['MLL1', 'MLL2', 'MLL3', 'MLL4']].mean(axis=1,\n",
    "#                                                                      skipna=True)\n",
    "#\n",
    "# # temp['cb'].sum()\n",
    "# # output_df['MLL5'].sum()\n",
    "#\n",
    "# # Appending output df\n",
    "# if c == 1:\n",
    "#     output_df_all = output_df\n",
    "# else:\n",
    "#     output_df_all = output_df_all.append(output_df)\n",
    "#\n",
    "# if env != \"local\":\n",
    "#     logger.debug(\"Shape of output df after appending\")\n",
    "#     logger.debug(output_df.shape)\n",
    "\n",
    "##########End backtest loop here###########\n",
    "\n",
    "if env == \"local\":\n",
    "    output_df.to_csv(\n",
    "        cwd + 'output_df_' + reg + '_' + str(date.today()) + '.csv',\n",
    "        index=False)\n",
    "\n",
    "###Foramtting in o9 #####\n",
    "o9_df = pd.merge(output_df, time_df, how='left', on='Week')\n",
    "o9_df['Version'] = version\n",
    "\n",
    "# o9_df = o9_df[['Version', 'Customer', 'Location', 'EPM',\n",
    "#                'Item', 'WeekStr', 'MLL1', 'MLL2', 'MLL3', 'MLL4', 'MLL5']]\n",
    "\n",
    "o9_df = o9_df[['Version', 'Sales Org', 'Location',\n",
    "               'Item', 'WeekStr', 'xgb_rf', 'xgb', 'lgb', 'cb']]\n",
    "\n",
    "o9_df.rename({'Version': 'Version.[Version Name]',\n",
    "              'Sales Org': 'Sales Domain.[Sales Org]',\n",
    "              'Location': 'Location.[Location]', 'Item': 'Item.[Planning Item]',\n",
    "              'WeekStr': 'Time.[Week]', 'cb': 'MLL Model 1 Fcst HL',\n",
    "              'lgb': 'MLL Model 2 Fcst HL', 'xgb_rf': 'MLL Model 3 Fcst HL',\n",
    "              'xgb': 'MLL Model 4 Fcst HL'}, axis=1, inplace=True)\n",
    "\n",
    "# o9_df.columns = ['Version.[Version Name]', 'Sales Domain.[Sales Org]',\n",
    "#                  'Location.[Location]', 'Item.[Planning Item]',\n",
    "#                  'Time.[Week]', 'MLL Model 1 Fcst HL Lag 2', 'MLL Model 2 Fcst HL Lag 2',\n",
    "#                  'MLL Model 3 Fcst HL Lag 2', 'MLL Model 4 Fcst HL Lag 2']\n",
    "\n",
    "# o9_df_orig = o9_df.copy()\n",
    "# o9_df['Item.[Planning Item]'] = o9_df['Item.[Planning Item]'].str.zfill(18)\n",
    "# o9_df['Location.[Location]'] = o9_df['Location.[Location]'].str.zfill(4)\n",
    "\n",
    "if env != \"local\":\n",
    "    logger.debug(\"Shape of o9 df\")\n",
    "    logger.debug(o9_df.shape)\n",
    "    logger.debug(\"o9_df head\")\n",
    "    logger.debug(o9_df.head(10))\n",
    "    logger.debug(o9_df[(o9_df['Time.[Week]'] == '2022-W46') & (\n",
    "                o9_df['Item.[Planning Item]'] == '000000447000198300') & (\n",
    "                                   o9_df['Location.[Location]'] == '0457')])\n",
    "\n",
    "if env == \"local\":\n",
    "    o9_df.to_csv(\n",
    "        cwd + 'Fact.MLL_HL_45' + reg + '_' + str(date.today()) + '.csv',\n",
    "        index=False)\n",
    "else:\n",
    "    O9DataLake.put(\"o9_df\", o9_df)\n",
    "\n",
    "###################################################\n",
    "if env != \"local\":\n",
    "    logger.debug(\"Script completed!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "[ProphesyInt] Tenant Conda Environment",
   "language": "python",
   "name": "genieaz_prophesyint"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "notebook_dict": {
   "ClassName": "o9.GraphCube.Plugins.Python.PythonScript",
   "InstanceName": "KHC_MLL_HL_Fwd_Final",
   "SliceKeys": [
    {
     "AttributeName": "L6",
     "DimensionName": "Item"
    }
   ],
   "file_path": "loaded_notebooks/KHC_MLL_HL_Fwd_Final.ipynb",
   "o9_selected_plugin_id": 203330
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
